"""
article that mentions imageJ http://dusty.physics.uiowa.edu/~goree/papers/feng-RSI-pixel-locking-2007.pdf
over arching thoughts on algorithm.

There are 3 stages in the processing of the data.
step 1 is taking each frame and generating a list of x-y coordinates for each of the beads in the frame.
step 2 is taking adjacent frames and determining which beads are related to each other through time.
step 3 is analyzing the collection of trajectories.

step 2 is dominated by adjacency lookups, which normally suggests a quad tree, however, quadtrees are efficient for dealing with unstructured data, we clearly have a lattice. Using a hash that has as tuples the bottom right coordinates of a square evenly tiled across the field might work quite well. Or even the bravais lattice coordinates in a hexagonal plane.
compared with the naive approach of checking the n bodies on an adjacent slide for which one is closest.

Using imageJ to spot check things
  count functionality read at 26k centers
  one rough center is (22000, 4800)
  adjacent center is (20000, 3900)
  out of a total field size of 160000,90000
  area = 14400000000
  seperation vector = 2000, 900
  distance = (2000**2+ 900**2)**.5 ~=2193
  cell radius = 2193/2.0 = 1096
  cell area = 1096**2*3.14 = 3771818.24
  approximate cells in full image= area/cell area ~= 3817
  misusing count then, clearly.
working imageJ flow(artisinal)
  open file
  set threshold, auto works fine so far, hit apply
  process->find maxima results in 3027, which is more in accordance of the rough estimate
  list results, save
wrote ijm macro in imageJ exploration
  parsing 5500 images took about 4 minutes
  sooo, that's good.

things to do with imageJ generated lists.
  √write thing to load the into python lists.
  strand matching
    we have N files with An points described in them.
      points are described with 3-4 numbers, x-y-time and maybe path-id (800x600 max)
    An-1 and An are not necessarily equal.
    we describe two particles as being the same if in An there is only one particle within a certain distance in An+1. Otherwise it doesn't count.
    so we have M paths
      say we have an array (paths) of arrays which represent the paths. this is initialized with just all of file A0 and is assembled as we go along.
      we have an array (frontier) which has indices for the most recent paths. This is initialized as range(len(A0))
      √we have an array (bins) of arrays which we insert the next set of points into
        √can be generated by [[[] for n in range(k)] for m in range(j)], will have j rows, with k columns.
        choose j and k such that bins represent some pixel size section aXb, so for particle particle P
          bins[Py/a][Px/b].append(P)
          one expects any individual bin to only have one member in it.
      on each new slide,
        we populate bins
        we run through frontier
          check where the last point on the associated path was (terminus)
          find the index for the terminus and poll that bin and it's adjacent bins.
          determine if there is 1 (only) member within a certain distance from the terminus
            if so:
              set it to the new terminus in that path
              pop it from bins.
            if not:
              remove the terminus path-id from the frontier
        once finished finding all satisfactory next steps for the frontier, collapse bins to make orphaned_points
        each point in orphaned_points is a the origin and terminus for a new path_id and is added to frontier.
        move on to next slide.
      after all slides, paths should contain all the points collected into continuous paths that satisfy the criterion.
estimates:
  since each slide will have a frontier equal to the length of the previous slide,
  and each member of the frontier has relatively consistently small amount of work when it's looked at
  I think this algorithm is roughly M*N_bar where M is the number of slides and N_bar is the average number of particles in a slide. Compared with the brute force approach which is M*N_bar^2 (historical times for this algorithm are 8-12 hours)
  my biggest concern is the flatten step, if bins is too large, it may take a long time to flatten it, so it's important to select bin spacing that keeps vacancy a small factor, maybe less than 50 percent if possible.
  as it stands, the test case with ~5k slides at ~3k points a piece means I've got about 15M points to work with, as I have many gigs of ram, I can probably store that all in ram. also, ~15M opps vs ~45G opps

choosing bin size:
  xs range from 0-800
  ys range from 0-600
  occupancy should hover near 1 with a preference for empty rather than doubles
  roughly 3000 to accomodate
  square cells make for easier math.
  (800/l)*(600/l)=4000
  6*8E4/4000=l^2
  ((6*8E4)/n)^.5=l where n is the number of cells
  for n in range(3000, 5000):
    l = ((6.0*8*10000)/n)**0.5
    if l % 1 == 0:
      print l, n
  the only integer value that works out nice and smooth is l = 10 for 4800 cells, which kind of makes sense in retrospect

timing measurements
  pop and contents array
  48472
--- 53.5086579323 seconds ---
50047146
424076272
--- 3.30963897705 seconds ---
  next(f) and generators
  48472
--- 53.204636097 seconds ---
50047146
424076272
--- 3.48503994942 seconds ---

making and collapsing 5500 utilities.empty_bins(60, 80)
--- 5.77174186707 seconds ---
--- 0.00104940761219 seconds per cycle---
--- 3.49802537398e-07 seconds per particle---

making and collapsing utilities.empty_bins(n, m) n*m 1000 times
--- 205.596997976 seconds 480000 bins --- 600, 800  l= 1
--- 47.3756978512 seconds 120000 bins --- 300, 400 l= 2
--- 16.9386630058 seconds 53400 bins --- l= 3
--- 7.98353004456 seconds 30000 bins --- l= 4
--- 4.40636205673 seconds 19200 bins --- l= 5
--- 3.04176902771 seconds 13400 bins --- l= 6
--- 2.20335888863 seconds 9890 bins --- l= 7
--- 1.64327692986 seconds 7500 bins --- l= 8
--- 1.30016207695 seconds 5963 bins --- l= 9
--- 1.02439808846 seconds 4800 bins --- 60,80 l= 10

path integrity metric
  break up space into bins of scale time step
  for each path stitched together
    calculate length of path
    go through each location
      calculate bin membership
      add path length to bin
heating run:
imageJ leg started at 12:24
done at 12:31
horrible things happened with heating lasers illuminating space differently
51263622 paths detected, because about a fourth of the slides had 50k particles.

experiment -> produces video file
video file -> tif images
tif -> coordinate slides
coordinate slides -> particle paths
particle paths -> velocity distribution (delta-r / delta-t)
fengs art
distinguish between too many solutions and no solutions

new imageJ
threshold
set measurement to find center of measurements
analyze particles
turn off scale

validations
  even distribution of least significant digits
  gaussian V-distribution with slightly fattened tails
  feng's papers


"""
